{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "ioos_tools = os.path.join(os.path.pardir)\n",
    "sys.path.append(ioos_tools)\n",
    "\n",
    "# Suppresing warnings for a \"pretty output.\"\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "\n",
    "# Specify a YYYY-MM-DD hh:mm:ss date or integer day offset.\n",
    "# If both start and stop are offsets they will be computed relative to datetime.today() at midnight.\n",
    "# Use the dates commented below to reproduce the last Boston Light Swim event forecast.\n",
    "date:\n",
    "    start: -4\n",
    "    stop: +3\n",
    "\n",
    "run_name: 'latest'\n",
    "\n",
    "# Boston harbor.\n",
    "region:\n",
    "    bbox: [-87.40, 24.25, -74.70, 36.70]\n",
    "    crs: 'urn:ogc:def:crs:OGC:1.3:CRS84'\n",
    "\n",
    "sos_name: 'water_surface_height_above_reference_datum'\n",
    "\n",
    "cf_names:\n",
    "    - sea_surface_height\n",
    "    - sea_surface_elevation\n",
    "    - sea_surface_height_above_geoid\n",
    "    - sea_surface_height_above_sea_level\n",
    "    - water_surface_height_above_reference_datum\n",
    "    - sea_surface_height_above_reference_ellipsoid\n",
    "\n",
    "units: 'meters'\n",
    "\n",
    "catalogs:\n",
    "    - https://data.ioos.us/csw\n",
    "    - https://gamone.whoi.edu/csw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data inside directory /home/filipe/IOOS/notebooks_demos/notebooks/latest\n",
      "*********************** Run information ************************\n",
      "Run date: 2017-07-05 14:15:48\n",
      "Start: 2017-07-01 00:00:00\n",
      "Stop: 2017-07-08 00:00:00\n",
      "Bounding box: -87.40, 24.25,-74.70, 36.70\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "from ioos_tools.ioos import parse_config\n",
    "\n",
    "config = parse_config('config.yaml')\n",
    "\n",
    "# Saves downloaded data into a temporary directory.\n",
    "save_dir = os.path.abspath(config['run_name'])\n",
    "if os.path.exists(save_dir):\n",
    "    shutil.rmtree(save_dir)\n",
    "os.makedirs(save_dir)\n",
    "\n",
    "fmt = '{:*^64}'.format\n",
    "print(fmt('Saving data inside directory {}'.format(save_dir)))\n",
    "print(fmt(' Run information '))\n",
    "print('Run date: {:%Y-%m-%d %H:%M:%S}'.format(datetime.utcnow()))\n",
    "print('Start: {:%Y-%m-%d %H:%M:%S}'.format(config['date']['start']))\n",
    "print('Stop: {:%Y-%m-%d %H:%M:%S}'.format(config['date']['stop']))\n",
    "print('Bounding box: {0:3.2f}, {1:3.2f},'\n",
    "      '{2:3.2f}, {3:3.2f}'.format(*config['region']['bbox']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_filter(config):\n",
    "    from owslib import fes\n",
    "    from ioos_tools.ioos import fes_date_filter\n",
    "    kw = dict(wildCard='*', escapeChar='\\\\',\n",
    "              singleChar='?', propertyname='apiso:AnyText')\n",
    "\n",
    "    or_filt = fes.Or([fes.PropertyIsLike(literal=('*%s*' % val), **kw)\n",
    "                      for val in config['cf_names']])\n",
    "\n",
    "    not_filt = fes.Not([fes.PropertyIsLike(literal='GRIB-2', **kw)])\n",
    "\n",
    "    begin, end = fes_date_filter(config['date']['start'],\n",
    "                                 config['date']['stop'])\n",
    "    bbox_crs = fes.BBox(config['region']['bbox'],\n",
    "                        crs=config['region']['crs'])\n",
    "    filter_list = [fes.And([bbox_crs, begin, end, or_filt, not_filt])]\n",
    "    return filter_list\n",
    "\n",
    "\n",
    "filter_list = make_filter(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************* Catalog information **********************\n",
      "URL: https://data.ioos.us/csw\n",
      "HTTPSConnectionPool(host='data.ioos.us', port=443): Max retries exceeded with url: /csw?service=CSW&version=2.0.2&request=GetCapabilities (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f60195d45f8>: Failed to establish a new connection: [Errno 111] Connection refused',))\n",
      "********************* Catalog information **********************\n",
      "URL: https://gamone.whoi.edu/csw\n",
      "Number of datasets available: 2\n",
      "COAWST Forecast System : USGS : US East Coast and Gulf of Mexico (Experimental)\n",
      "COAWST Modeling System: USEast: ROMS-WRF-SWAN coupled model (aka CNAPS)\n",
      "***************************** DAP ******************************\n",
      "http://geoport-dev.whoi.edu/thredds/dodsC/coawst_4/use/fmrc/coawst_4_use_best.ncd.html\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ioos_tools.ioos import service_urls, get_csw_records\n",
    "from owslib.csw import CatalogueServiceWeb\n",
    "\n",
    "\n",
    "dap_urls = []\n",
    "sos_urls = []\n",
    "erddap_urls = []\n",
    "\n",
    "for endpoint in config['catalogs']:\n",
    "    print(fmt(' Catalog information '))\n",
    "    print('URL: {}'.format(endpoint))\n",
    "    try:\n",
    "        csw = CatalogueServiceWeb(endpoint, timeout=120)\n",
    "    except Exception as e:\n",
    "        print('{}'.format(e))\n",
    "        continue\n",
    "    csw = get_csw_records(csw, filter_list, esn='full')\n",
    "    # OPeNDAP\n",
    "    OPeNDAP = service_urls(csw.records, identifier='OPeNDAP:OPeNDAP')\n",
    "    odp = service_urls(csw.records, identifier='urn:x-esri:specification:ServiceType:odp:url')\n",
    "    dap = OPeNDAP + odp\n",
    "    dap_urls.extend(dap)\n",
    "    # ERDDAP\n",
    "    ERDDAP = service_urls(csw.records, identifier='ERDDAP:tabledap')\n",
    "    erddap_urls.extend(ERDDAP)\n",
    "    # SOS\n",
    "    SOS = service_urls(csw.records, identifier='OGC:SOS')\n",
    "    sos_urls.extend(SOS)\n",
    "\n",
    "    print('Number of datasets available: {}'.format(len(csw.records.keys())))\n",
    "\n",
    "    for rec, item in csw.records.items():\n",
    "        print('{}'.format(item.title))\n",
    "    if dap:\n",
    "        print(fmt(' DAP '))\n",
    "        for url in dap:\n",
    "            print('{}.html'.format(url))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get only unique endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dap_urls = list(set(dap_urls))\n",
    "sos_urls = list(set(sos_urls))\n",
    "erddap_urls = list(set(erddap_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ioos_tools.ioos import is_station\n",
    "\n",
    "# Filter out some station endpoints.\n",
    "non_stations = []\n",
    "for url in dap_urls:\n",
    "    try:\n",
    "        if not is_station(url):\n",
    "            non_stations.append(url)\n",
    "    except (RuntimeError, OSError, IOError) as e:\n",
    "        print('Could not access URL {}. {!r}'.format(url, e))\n",
    "\n",
    "dap_urls = non_stations\n",
    "\n",
    "print(fmt(' Filtered DAP '))\n",
    "for url in dap_urls:\n",
    "    print('{}.html'.format(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `pyoos` collectors for `NdbcSos`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyoos.collectors.ndbc.ndbc_sos import NdbcSos\n",
    "\n",
    "collector_ndbc = NdbcSos()\n",
    "\n",
    "collector_ndbc.set_bbox(config['region']['bbox'])\n",
    "collector_ndbc.end_time = config['date']['stop']\n",
    "collector_ndbc.start_time = config['date']['start']\n",
    "collector_ndbc.variables = [config['sos_name']]\n",
    "\n",
    "ofrs = collector_ndbc.server.offerings\n",
    "title = collector_ndbc.server.identification.title\n",
    "print(fmt(' NDBC Collector offerings '))\n",
    "print('{}: {} offerings'.format(title, len(ofrs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ioos_tools.ioos import collector2table\n",
    "\n",
    "ndbc = collector2table(collector=collector_ndbc,\n",
    "                       config=config,\n",
    "                       col='sea_water_temperature (C)')\n",
    "\n",
    "if ndbc:\n",
    "    data = dict(\n",
    "        station_name=[s._metadata.get('station_name') for s in ndbc],\n",
    "        station_code=[s._metadata.get('station_code') for s in ndbc],\n",
    "        sensor=[s._metadata.get('sensor') for s in ndbc],\n",
    "        lon=[s._metadata.get('lon') for s in ndbc],\n",
    "        lat=[s._metadata.get('lat') for s in ndbc],\n",
    "        depth=[s._metadata.get('depth') for s in ndbc],\n",
    "    )\n",
    "\n",
    "table = pd.DataFrame(data).set_index('station_code')\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and `CoopsSos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyoos.collectors.coops.coops_sos import CoopsSos\n",
    "\n",
    "collector_coops = CoopsSos()\n",
    "\n",
    "collector_coops.set_bbox(config['region']['bbox'])\n",
    "collector_coops.end_time = config['date']['stop']\n",
    "collector_coops.start_time = config['date']['start']\n",
    "collector_coops.variables = [config['sos_name']]\n",
    "\n",
    "ofrs = collector_coops.server.offerings\n",
    "title = collector_coops.server.identification.title\n",
    "print(fmt(' Collector offerings '))\n",
    "print('{}: {} offerings'.format(title, len(ofrs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coops = collector2table(collector=collector_coops,\n",
    "                        config=config,\n",
    "                        col='sea_water_temperature (C)')\n",
    "\n",
    "if coops:\n",
    "    data = dict(\n",
    "        station_name=[s._metadata.get('station_name') for s in coops],\n",
    "        station_code=[s._metadata.get('station_code') for s in coops],\n",
    "        sensor=[s._metadata.get('sensor') for s in coops],\n",
    "        lon=[s._metadata.get('lon') for s in coops],\n",
    "        lat=[s._metadata.get('lat') for s in coops],\n",
    "        depth=[s._metadata.get('depth') for s in coops],\n",
    "    )\n",
    "\n",
    "table = pd.DataFrame(data).set_index('station_code')\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will join all the observations into an uniform series, interpolated to 1-hour interval, for the model-data comparison.\n",
    "\n",
    "This step is necessary because the observations can be 7 or 10 minutes resolution,\n",
    "while the models can be 30 to 60 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = ndbc + coops\n",
    "\n",
    "index = pd.date_range(start=config['date']['start'].replace(tzinfo=None),\n",
    "                      end=config['date']['stop'].replace(tzinfo=None),\n",
    "                      freq='1H')\n",
    "\n",
    "# Preserve metadata with `reindex`.\n",
    "observations = []\n",
    "for series in data:\n",
    "    _metadata = series._metadata\n",
    "    obs = series.reindex(index=index, limit=1, method='nearest')\n",
    "    obs._metadata = _metadata\n",
    "    observations.append(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next cell we will save the data for quicker access later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import iris\n",
    "from ioos_tools.tardis import series2cube\n",
    "\n",
    "attr = dict(\n",
    "    featureType='timeSeries',\n",
    "    Conventions='CF-1.6',\n",
    "    standard_name_vocabulary='CF-1.6',\n",
    "    cdm_data_type='Station',\n",
    "    comment='Data from http://opendap.co-ops.nos.noaa.gov'\n",
    ")\n",
    "\n",
    "\n",
    "cubes = iris.cube.CubeList(\n",
    "    [series2cube(obs, attr=attr) for obs in observations]\n",
    ")\n",
    "\n",
    "outfile = os.path.join(save_dir, 'OBS_DATA.nc')\n",
    "iris.save(cubes, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to loop the models we found above,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from iris.exceptions import (CoordinateNotFoundError, ConstraintMismatchError,\n",
    "                             MergeError)\n",
    "from ioos_tools.ioos import get_model_name\n",
    "from ioos_tools.tardis import quick_load_cubes, proc_cube, is_model, get_surface\n",
    "\n",
    "print(fmt(' Models '))\n",
    "cubes = dict()\n",
    "for k, url in enumerate(dap_urls):\n",
    "    print('\\n[Reading url {}/{}]: {}'.format(k+1, len(dap_urls), url))\n",
    "    try:\n",
    "        cube = quick_load_cubes(url, config['cf_names'],\n",
    "                                callback=None, strict=True)\n",
    "        if is_model(cube):\n",
    "            cube = proc_cube(cube,\n",
    "                             bbox=config['region']['bbox'],\n",
    "                             time=(config['date']['start'],\n",
    "                                   config['date']['stop']),\n",
    "                             units=config['units'])\n",
    "        else:\n",
    "            print('[Not model data]: {}'.format(url))\n",
    "            continue\n",
    "        cube = get_surface(cube)\n",
    "        mod_name = get_model_name(url)\n",
    "        cubes.update({mod_name: cube})\n",
    "    except (RuntimeError, ValueError,\n",
    "            ConstraintMismatchError, CoordinateNotFoundError,\n",
    "            IndexError) as e:\n",
    "        print('Cannot get cube for: {}\\n{}'.format(url, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will match them with the nearest observed time-series. The `max_dist=0.08` is in degrees, that is roughly 8 kilometers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import iris\n",
    "from iris.pandas import as_series\n",
    "from ioos_tools.tardis import (make_tree, get_nearest_water,\n",
    "                               add_station, ensure_timeseries, remove_ssh)\n",
    "\n",
    "for mod_name, cube in cubes.items():\n",
    "    fname = '{}.nc'.format(mod_name)\n",
    "    fname = os.path.join(save_dir, fname)\n",
    "    print(fmt(' Downloading to file {} '.format(fname)))\n",
    "    try:\n",
    "        tree, lon, lat = make_tree(cube)\n",
    "    except CoordinateNotFoundError as e:\n",
    "        print('Cannot make KDTree for: {}'.format(mod_name))\n",
    "        continue\n",
    "    # Get model series at observed locations.\n",
    "    raw_series = dict()\n",
    "    for obs in observations:\n",
    "        obs = obs._metadata\n",
    "        station = obs['station_code']\n",
    "        try:\n",
    "            kw = dict(k=10, max_dist=0.08, min_var=0.01)\n",
    "            args = cube, tree, obs['lon'], obs['lat']\n",
    "            try:\n",
    "                series, dist, idx = get_nearest_water(*args, **kw)\n",
    "            except RuntimeError as e:\n",
    "                print('Cannot download {!r}.\\n{}'.format(cube, e))\n",
    "                series = None\n",
    "        except ValueError as e:\n",
    "            status = 'No Data'\n",
    "            print('[{}] {}'.format(status, obs['station_name']))\n",
    "            continue\n",
    "        if not series:\n",
    "            status = 'Land   '\n",
    "        else:\n",
    "            raw_series.update({station: series})\n",
    "            series = as_series(series)\n",
    "            status = 'Water  '\n",
    "        print('[{}] {}'.format(status, obs['station_name']))\n",
    "    if raw_series:  # Save cube.\n",
    "        for station, cube in raw_series.items():\n",
    "            cube = add_station(cube, station)\n",
    "            cube = remove_ssh(cube)\n",
    "        try:\n",
    "            cube = iris.cube.CubeList(raw_series.values()).merge_cube()\n",
    "        except MergeError as e:\n",
    "            print(e)\n",
    "        ensure_timeseries(cube)\n",
    "        try:\n",
    "            iris.save(cube, fname)\n",
    "        except AttributeError:\n",
    "            # FIXME: we should patch the bad attribute instead of removing everything.\n",
    "            cube.attributes = {}\n",
    "            iris.save(cube, fname)\n",
    "        del cube\n",
    "    print('Finished processing [{}]'.format(mod_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is possible to compute some simple comparison metrics. First we'll calculate the model mean bias:\n",
    "\n",
    "$$ \\text{MB} = \\mathbf{\\overline{m}} - \\mathbf{\\overline{o}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ioos_tools.ioos import stations_keys\n",
    "\n",
    "\n",
    "def rename_cols(df, config):\n",
    "    cols = stations_keys(config, key='station_name')\n",
    "    return df.rename(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ioos_tools.ioos import load_ncs\n",
    "from ioos_tools.skill_score import mean_bias, apply_skill\n",
    "\n",
    "dfs = load_ncs(config)\n",
    "\n",
    "df = apply_skill(dfs, mean_bias, remove_mean=False, filter_tides=False)\n",
    "skill_score = dict(mean_bias=df.to_dict())\n",
    "\n",
    "# Filter out stations with no valid comparison.\n",
    "df.dropna(how='all', axis=1, inplace=True)\n",
    "df = df.applymap('{:.2f}'.format).replace('nan', '--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the root mean squared rrror of the deviations from the mean:\n",
    "$$ \\text{CRMS} = \\sqrt{\\left(\\mathbf{m'} - \\mathbf{o'}\\right)^2}$$\n",
    "\n",
    "where: $\\mathbf{m'} = \\mathbf{m} - \\mathbf{\\overline{m}}$ and $\\mathbf{o'} = \\mathbf{o} - \\mathbf{\\overline{o}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ioos_tools.skill_score import rmse\n",
    "\n",
    "dfs = load_ncs(config)\n",
    "\n",
    "df = apply_skill(dfs, rmse, remove_mean=True, filter_tides=False)\n",
    "skill_score['rmse'] = df.to_dict()\n",
    "\n",
    "# Filter out stations with no valid comparison.\n",
    "df.dropna(how='all', axis=1, inplace=True)\n",
    "df = df.applymap('{:.2f}'.format).replace('nan', '--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Stringfy keys.\n",
    "for key in skill_score.keys():\n",
    "    skill_score[key] = {str(k): v for k, v in skill_score[key].items()}\n",
    "\n",
    "mean_bias = pd.DataFrame.from_dict(skill_score['mean_bias'])\n",
    "mean_bias = mean_bias.applymap('{:.2f}'.format).replace('nan', '--')\n",
    "\n",
    "skill_score = pd.DataFrame.from_dict(skill_score['rmse'])\n",
    "skill_score = skill_score.applymap('{:.2f}'.format).replace('nan', '--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ioos_tools.ioos import make_map\n",
    "\n",
    "bbox = config['region']['bbox']\n",
    "units = config['units']\n",
    "run_name = config['run_name']\n",
    "\n",
    "kw = dict(zoom_start=11, line=True, states=False,\n",
    "          secoora_stations=False, layers=False)\n",
    "m = make_map(bbox, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_obs = stations_keys(config)\n",
    "\n",
    "from glob import glob\n",
    "from operator import itemgetter\n",
    "\n",
    "import iris\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "iris.FUTURE.netcdf_promote = True\n",
    "\n",
    "big_list = []\n",
    "for fname in glob(os.path.join(save_dir, '*.nc')):\n",
    "    if 'OBS_DATA' in fname:\n",
    "        continue\n",
    "    cube = iris.load_cube(fname)\n",
    "    model = os.path.split(fname)[1].split('-')[-1].split('.')[0]\n",
    "    lons = cube.coord(axis='X').points\n",
    "    lats = cube.coord(axis='Y').points\n",
    "    stations = cube.coord('station_code').points\n",
    "    models = [model]*lons.size\n",
    "    lista = zip(models, lons.tolist(), lats.tolist(), stations.tolist())\n",
    "    big_list.extend(lista)\n",
    "\n",
    "big_list.sort(key=itemgetter(3))\n",
    "df = pd.DataFrame(big_list, columns=['name', 'lon', 'lat', 'station'])\n",
    "df.set_index('station', drop=True, inplace=True)\n",
    "groups = df.groupby(df.index)\n",
    "\n",
    "\n",
    "locations, popups = [], []\n",
    "for station, info in groups:\n",
    "    sta_name = all_obs[station]\n",
    "    for lat, lon, name in zip(info.lat, info.lon, info.name):\n",
    "        locations.append([lat, lon])\n",
    "        popups.append('[{}]: {}'.format(name, sta_name))\n",
    "\n",
    "MarkerCluster(locations=locations, popups=popups).add_to(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = {\n",
    "    'coawst_4_use_best': 'COAWST_4',\n",
    "    'global': 'HYCOM',\n",
    "    'NECOFS_GOM3_FORECAST': 'NECOFS_GOM3',\n",
    "    'NECOFS_FVCOM_OCEAN_MASSBAY_FORECAST': 'NECOFS_MassBay',\n",
    "    'OBS_DATA': 'Observations'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bokeh.resources import CDN\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.embed import file_html\n",
    "from bokeh.models import HoverTool\n",
    "from itertools import cycle\n",
    "from bokeh.palettes import Spectral6\n",
    "\n",
    "from folium import IFrame\n",
    "\n",
    "# Plot defaults.\n",
    "colors = Spectral6\n",
    "colorcycler = cycle(colors)\n",
    "tools = 'pan,box_zoom,reset'\n",
    "width, height = 750, 250\n",
    "\n",
    "\n",
    "def make_plot(df, station):\n",
    "    p = figure(toolbar_location='above',\n",
    "               x_axis_type='datetime',\n",
    "               width=width,\n",
    "               height=height,\n",
    "               tools=tools,\n",
    "               title=str(station))\n",
    "    for column, series in df.iteritems():\n",
    "        series.dropna(inplace=True)\n",
    "        if not series.empty:\n",
    "            line = p.line(\n",
    "                x=series.index,\n",
    "                y=series.values,\n",
    "                legend='%s' % titles.get(column, column),\n",
    "                line_color=next(colorcycler),\n",
    "                line_width=5,\n",
    "                line_cap='round',\n",
    "                line_join='round'\n",
    "            )\n",
    "            if 'OBS_DATA' not in column:\n",
    "                bias = mean_bias[str(station)][column]\n",
    "                skill = skill_score[str(station)][column]\n",
    "            else:\n",
    "                skill = bias = 'NA'\n",
    "            p.add_tools(HoverTool(tooltips=[('Name', '%s' % column),\n",
    "                                            ('Bias', bias),\n",
    "                                            ('Skill', skill)],\n",
    "                                  renderers=[line]))\n",
    "    return p\n",
    "\n",
    "\n",
    "def make_marker(p, station):\n",
    "    lons = stations_keys(config, key='lon')\n",
    "    lats = stations_keys(config, key='lat')\n",
    "\n",
    "    lon, lat = lons[station], lats[station]\n",
    "    html = file_html(p, CDN, station)\n",
    "    iframe = IFrame(html, width=width+40, height=height+80)\n",
    "\n",
    "    popup = folium.Popup(iframe, max_width=2650)\n",
    "    icon = folium.Icon(color='green', icon='stats')\n",
    "    marker = folium.Marker(location=[lat, lon],\n",
    "                           popup=popup,\n",
    "                           icon=icon)\n",
    "    return marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs = load_ncs(config)\n",
    "\n",
    "for station in dfs:\n",
    "    sta_name = all_obs[station]\n",
    "    df = dfs[station]\n",
    "    if df.empty:\n",
    "        continue\n",
    "    p = make_plot(df, station)\n",
    "    maker = make_marker(p, station)\n",
    "    maker.add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "ax = dfs[dfs.items[-1]].plot(figsize=(9, 3.25)).legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
